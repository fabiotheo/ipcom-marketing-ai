"""OSP Marketing Tools server implementation."""

import os
import asyncio
import json
import re
from typing import Dict, Any, List
from datetime import datetime
from collections import OrderedDict
from concurrent.futures import ThreadPoolExecutor

from mcp.server.fastmcp import FastMCP
from mcp.types import TextContent

from .config import Config
from .analysis import analyze_content_with_frameworks

def get_logger(name: str):
    """Configure structured logging for OSP Marketing Tools."""
    import logging
    import sys

    logger = logging.getLogger(name)

    # Evitar configuração duplicada
    if not logger.handlers:
        # Configure handler
        handler = logging.StreamHandler(sys.stdout)

        # Structured logging format
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - '
            '[%(funcName)s:%(lineno)d] - %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        handler.setFormatter(formatter)

        logger.addHandler(handler)
        logger.setLevel(getattr(logging, Config.LOG_LEVEL, logging.INFO))

        # Prevent propagation to avoid duplicate logs
        logger.propagate = False

    return logger

logger = get_logger(__name__)

# Custom exceptions for better error handling
class OSPToolsError(Exception):
    """Base exception for OSP Marketing Tools."""
    pass

class ContentValidationError(OSPToolsError):
    """Raised when content validation fails."""
    pass

class FrameworkValidationError(OSPToolsError):
    """Raised when framework validation fails."""
    pass

class CacheError(OSPToolsError):
    """Raised when cache operations fail."""
    pass

class FileOperationError(OSPToolsError):
    """Raised when file operations fail."""
    pass

def handle_exceptions(func):
    """Decorator to handle common exceptions in tool functions."""
    async def wrapper(*args, **kwargs):
        try:
            return await func(*args, **kwargs)
        except ContentValidationError as e:
            logger.warning(f"Content validation error in {func.__name__}: {str(e)}")
            return {
                "success": False,
                "error": f"Content validation failed: {str(e)}",
                "error_type": "content_validation",
                "tool": func.__name__
            }
        except FrameworkValidationError as e:
            logger.warning(f"Framework validation error in {func.__name__}: {str(e)}")
            return {
                "success": False,
                "error": f"Framework validation failed: {str(e)}",
                "error_type": "framework_validation",
                "tool": func.__name__
            }
        except FileOperationError as e:
            logger.error(f"File operation error in {func.__name__}: {str(e)}")
            return {
                "success": False,
                "error": f"File operation failed: {str(e)}",
                "error_type": "file_operation",
                "tool": func.__name__
            }
        except CacheError as e:
            logger.error(f"Cache error in {func.__name__}: {str(e)}")
            return {
                "success": False,
                "error": f"Cache operation failed: {str(e)}",
                "error_type": "cache_operation",
                "tool": func.__name__
            }
        except Exception as e:
            logger.error(f"Unexpected error in {func.__name__}: {str(e)}")
            return {
                "success": False,
                "error": f"Unexpected error occurred: {str(e)}",
                "error_type": "unexpected",
                "tool": func.__name__
            }
    return wrapper

# Create server instance using FastMCP
mcp = FastMCP("osp_marketing_tools")

# Sistema de Versionamento para Metodologias
METHODOLOGY_VERSIONS = {
    "osp_editing_codes": "1.0.0",
    "osp_writing_guide": "1.0.0",
    "osp_meta_guide": "1.0.0",
    "osp_value_map_guide": "1.0.0",
    "osp_seo_guide": "1.0.0",
    "frameworks_marketing_2025": "2025.1",
    "technical_writing_2025": "2025.1",
    "seo_frameworks_2025": "2025.1"
}

# Cache LRU (Least Recently Used) para conteúdo com limite de tamanho e estatísticas
class LRUCache:
    def __init__(self, max_size: int = Config.CACHE_MAX_SIZE):
        self.max_size = max_size
        self.cache = OrderedDict()

        # Estatísticas de cache
        self.stats = {
            "hits": 0,
            "misses": 0,
            "evictions": 0,
            "total_requests": 0
        }

    def get(self, key: str):
        self.stats["total_requests"] += 1

        if key in self.cache:
            # Cache hit - move to end (most recently used)
            self.cache.move_to_end(key)
            self.stats["hits"] += 1
            return self.cache[key]

        # Cache miss
        self.stats["misses"] += 1
        return None

    def set(self, key: str, value: Any):
        if key in self.cache:
            # Update existing key
            self.cache.move_to_end(key)
        elif len(self.cache) >= self.max_size:
            # Remove least recently used item (eviction)
            self.cache.popitem(last=False)
            self.stats["evictions"] += 1

        self.cache[key] = value

    def get_stats(self) -> Dict[str, Any]:
        """Retorna estatísticas detalhadas do cache."""
        total_requests = self.stats["total_requests"]
        hit_ratio = (self.stats["hits"] / total_requests * 100) if total_requests > 0 else 0

        return {
            "cache_size": len(self.cache),
            "max_size": self.max_size,
            "utilization": round((len(self.cache) / self.max_size) * 100, 1),
            "hit_ratio": round(hit_ratio, 1),
            "total_requests": total_requests,
            "hits": self.stats["hits"],
            "misses": self.stats["misses"],
            "evictions": self.stats["evictions"],
            "most_recent_keys": list(self.cache.keys())[-5:] if self.cache else []
        }

    def clear_stats(self):
        """Limpa as estatísticas do cache."""
        self.stats = {
            "hits": 0,
            "misses": 0,
            "evictions": 0,
            "total_requests": 0
        }

    def __contains__(self, key: str) -> bool:
        return key in self.cache

# Cache otimizado para conteúdo (configurável via environment)
CONTENT_CACHE = LRUCache(max_size=Config.CACHE_MAX_SIZE)

# Frameworks válidos para análise multi-framework
VALID_FRAMEWORKS = {"IDEAL", "STEPPS", "E-E-A-T", "GDocP"}

# ===== FUNÇÕES AUXILIARES DE ANÁLISE =====

def _calculate_score(indicators: int, max_indicators: int = 10) -> float:
    """Calcula score normalizado entre 0-100."""
    return min(100, round((indicators / max_indicators) * 100, 1))

# IDEAL Framework Analysis Functions
def _analyze_target_identification(content: str) -> float:
    """Analisa identificação de público-alvo no conteúdo."""
    audience_keywords = ['users', 'developers', 'customers', 'team', 'audience', 'personas', 'stakeholders']
    pain_keywords = ['problem', 'challenge', 'issue', 'difficulty', 'struggle', 'pain point']

    audience_count = sum(1 for keyword in audience_keywords if keyword.lower() in content.lower())
    pain_count = sum(1 for keyword in pain_keywords if keyword.lower() in content.lower())

    return _calculate_score(audience_count + pain_count, 8)

def _extract_audience_signals(content: str) -> List[str]:
    """Extrai sinais de identificação de audiência."""
    signals = []
    patterns = [
        r'(?:for|aimed at|targeting)\s+(\w+(?:\s+\w+)*)',
        r'(\w+(?:\s+\w+)*)\s+(?:need|want|require)',
        r'if you(?:\s+are)?\s+(\w+(?:\s+\w+)*)'
    ]

    for pattern in patterns:
        matches = re.findall(pattern, content, re.IGNORECASE)
        signals.extend([match.strip() for match in matches[:3]])

    return signals[:5]

def _analyze_insight_discovery(content: str) -> float:
    """Analisa descoberta de insights únicos."""
    insight_keywords = ['insight', 'discovery', 'trend', 'pattern', 'finding', 'research', 'data shows']
    count = sum(1 for keyword in insight_keywords if keyword.lower() in content.lower())
    return _calculate_score(count, 6)

def _count_unique_insights(content: str) -> int:
    """Conta insights únicos baseados em palavras-chave."""
    insight_patterns = [
        r'(?:we found|research shows|data indicates|studies reveal)',
        r'(?:surprisingly|interestingly|notably)',
        r'(?:\d+%|\d+ percent) of'
    ]

    total = 0
    for pattern in insight_patterns:
        total += len(re.findall(pattern, content, re.IGNORECASE))

    return total

def _analyze_educational_value(content: str) -> float:
    """Analisa valor educacional do conteúdo."""
    educational_keywords = ['how to', 'tutorial', 'guide', 'learn', 'understand', 'explain', 'example']
    count = sum(1 for keyword in educational_keywords if keyword.lower() in content.lower())
    return _calculate_score(count, 7)

def _count_actionable_items(content: str) -> int:
    """Conta itens acionáveis no conteúdo."""
    actionable_patterns = [
        r'(?:step \d+|first|second|third|next|then)',
        r'(?:try|use|implement|apply|follow)',
        r'(?:action|todo|task|homework)'
    ]

    total = 0
    for pattern in actionable_patterns:
        total += len(re.findall(pattern, content, re.IGNORECASE))

    return total

def _analyze_cta_effectiveness(content: str) -> float:
    """Analisa efetividade de calls-to-action."""
    cta_keywords = ['sign up', 'download', 'subscribe', 'contact', 'get started', 'try now', 'learn more']
    count = sum(1 for keyword in cta_keywords if keyword.lower() in content.lower())
    return _calculate_score(count, 5)

def _count_calls_to_action(content: str) -> int:
    """Conta calls-to-action no conteúdo."""
    cta_patterns = [
        r'(?:click here|sign up|get started|download|subscribe)',
        r'(?:contact us|learn more|try now|book a call)',
        r'(?:register|join|follow|share)'
    ]

    total = 0
    for pattern in cta_patterns:
        total += len(re.findall(pattern, content, re.IGNORECASE))

    return total

def _analyze_feedback_opportunities(content: str) -> float:
    """Analisa oportunidades de feedback."""
    feedback_keywords = ['feedback', 'comment', 'review', 'suggestion', 'improvement', 'thoughts']
    count = sum(1 for keyword in feedback_keywords if keyword.lower() in content.lower())
    return _calculate_score(count, 4)

def _count_feedback_elements(content: str) -> int:
    """Conta elementos de feedback no conteúdo."""
    feedback_patterns = [
        r'(?:what do you think|share your thoughts|let us know)',
        r'(?:feedback|comments|suggestions)',
        r'(?:rate|review|evaluate)'
    ]

    total = 0
    for pattern in feedback_patterns:
        total += len(re.findall(pattern, content, re.IGNORECASE))

    return total

# STEPPS Framework Analysis Functions
def _analyze_social_currency(content: str) -> float:
    """Analisa moeda social do conteúdo."""
    social_keywords = ['exclusive', 'insider', 'secret', 'advanced', 'expert', 'professional', 'premium']
    count = sum(1 for keyword in social_keywords if keyword.lower() in content.lower())
    return _calculate_score(count, 6)

def _count_exclusive_insights(content: str) -> int:
    """Conta insights exclusivos."""
    exclusive_patterns = [
        r'(?:exclusive|insider|behind the scenes)',
        r'(?:only \d+%|few people|secret)',
        r'(?:advanced|expert level|professional)'
    ]

    total = 0
    for pattern in exclusive_patterns:
        total += len(re.findall(pattern, content, re.IGNORECASE))

    return total

def _analyze_memory_triggers(content: str) -> float:
    """Analisa gatilhos de memória."""
    trigger_keywords = ['remember', 'think of', 'whenever', 'always', 'every time', 'remind']
    count = sum(1 for keyword in trigger_keywords if keyword.lower() in content.lower())
    return _calculate_score(count, 5)

def _count_trigger_words(content: str) -> int:
    """Conta palavras gatilho."""
    trigger_patterns = [
        r'(?:remember|recall|think of)',
        r'(?:whenever|every time|always when)',
        r'(?:remind|association|connection)'
    ]

    total = 0
    for pattern in trigger_patterns:
        total += len(re.findall(pattern, content, re.IGNORECASE))

    return total

def _analyze_emotional_impact(content: str) -> float:
    """Analisa impacto emocional."""
    emotion_keywords = ['amazing', 'incredible', 'shocking', 'surprising', 'exciting', 'frustrating', 'love', 'hate']
    count = sum(1 for keyword in emotion_keywords if keyword.lower() in content.lower())
    return _calculate_score(count, 8)

def _count_emotional_language(content: str) -> int:
    """Conta linguagem emocional."""
    emotional_patterns = [
        r'(?:amazing|incredible|fantastic|awesome)',
        r'(?:shocking|surprising|unbelievable)',
        r'(?:love|hate|excited|frustrated)'
    ]

    total = 0
    for pattern in emotional_patterns:
        total += len(re.findall(pattern, content, re.IGNORECASE))

    return total

def _analyze_public_visibility(content: str) -> float:
    """Analisa visibilidade pública."""
    public_keywords = ['share', 'public', 'community', 'social', 'visible', 'showcase', 'display']
    count = sum(1 for keyword in public_keywords if keyword.lower() in content.lower())
    return _calculate_score(count, 6)

def _count_social_proof_elements(content: str) -> int:
    """Conta elementos de prova social."""
    social_proof_patterns = [
        r'(?:\d+(?:,\d+)*\s+(?:users|customers|people))',
        r'(?:testimonial|review|rating)',
        r'(?:trusted by|used by|chosen by)'
    ]

    total = 0
    for pattern in social_proof_patterns:
        total += len(re.findall(pattern, content, re.IGNORECASE))

    return total

def _analyze_practical_value(content: str) -> float:
    """Analisa valor prático."""
    practical_keywords = ['useful', 'practical', 'helpful', 'tip', 'trick', 'hack', 'solution']
    count = sum(1 for keyword in practical_keywords if keyword.lower() in content.lower())
    return _calculate_score(count, 7)

def _count_practical_tips(content: str) -> int:
    """Conta dicas práticas."""
    tip_patterns = [
        r'(?:tip|trick|hack|shortcut)',
        r'(?:how to|step by step|tutorial)',
        r'(?:practical|useful|helpful)'
    ]

    total = 0
    for pattern in tip_patterns:
        total += len(re.findall(pattern, content, re.IGNORECASE))

    return total

def _analyze_narrative_structure(content: str) -> float:
    """Analisa estrutura narrativa."""
    story_keywords = ['story', 'once', 'then', 'finally', 'character', 'journey', 'experience']
    count = sum(1 for keyword in story_keywords if keyword.lower() in content.lower())
    return _calculate_score(count, 6)

def _count_story_elements(content: str) -> int:
    """Conta elementos de história."""
    story_patterns = [
        r'(?:once upon|story|narrative)',
        r'(?:character|protagonist|hero)',
        r'(?:journey|experience|adventure)'
    ]

    total = 0
    for pattern in story_patterns:
        total += len(re.findall(pattern, content, re.IGNORECASE))

    return total

# E-E-A-T Framework Analysis Functions
def _analyze_first_hand_experience(content: str) -> float:
    """Analisa experiência em primeira pessoa."""
    experience_keywords = ['I have', 'we tested', 'my experience', 'in practice', 'personally', 'firsthand']
    count = sum(1 for keyword in experience_keywords if keyword.lower() in content.lower())
    return _calculate_score(count, 6)

def _count_experience_signals(content: str) -> int:
    """Conta sinais de experiência."""
    experience_patterns = [
        r'(?:I have|we tested|my experience)',
        r'(?:in practice|personally|firsthand)',
        r'(?:after \d+ years|having worked)'
    ]

    total = 0
    for pattern in experience_patterns:
        total += len(re.findall(pattern, content, re.IGNORECASE))

    return total

def _analyze_subject_expertise(content: str) -> float:
    """Analisa expertise no assunto."""
    expertise_keywords = ['expert', 'specialist', 'professional', 'certified', 'qualified', 'experienced']
    count = sum(1 for keyword in expertise_keywords if keyword.lower() in content.lower())
    return _calculate_score(count, 6)

def _measure_technical_depth(content: str) -> Dict[str, int]:
    """Mede profundidade técnica."""
    technical_indicators = {
        'code_blocks': len(re.findall(r'```[\s\S]*?```', content)),
        'technical_terms': len(re.findall(r'(?:API|SDK|JSON|HTTP|SQL|CSS|HTML|JavaScript)', content, re.IGNORECASE)),
        'numbers_data': len(re.findall(r'\d+(?:\.\d+)?%?', content))
    }
    return technical_indicators

def _analyze_authority_signals(content: str) -> float:
    """Analisa sinais de autoridade."""
    authority_keywords = ['authority', 'leader', 'recognized', 'award', 'published', 'speaker', 'consultant']
    count = sum(1 for keyword in authority_keywords if keyword.lower() in content.lower())
    return _calculate_score(count, 6)

def _count_authority_indicators(content: str) -> int:
    """Conta indicadores de autoridade."""
    authority_patterns = [
        r'(?:published|author|speaker)',
        r'(?:award|recognition|certified)',
        r'(?:leader|authority|expert)'
    ]

    total = 0
    for pattern in authority_patterns:
        total += len(re.findall(pattern, content, re.IGNORECASE))

    return total

def _analyze_trustworthiness(content: str) -> float:
    """Analisa confiabilidade."""
    trust_keywords = ['transparent', 'honest', 'verified', 'source', 'citation', 'reference', 'disclaimer']
    count = sum(1 for keyword in trust_keywords if keyword.lower() in content.lower())
    return _calculate_score(count, 7)

def _count_credibility_markers(content: str) -> int:
    """Conta marcadores de credibilidade."""
    credibility_patterns = [
        r'(?:source|citation|reference)',
        r'(?:verified|confirmed|validated)',
        r'(?:transparent|honest|disclaimer)'
    ]

    total = 0
    for pattern in credibility_patterns:
        total += len(re.findall(pattern, content, re.IGNORECASE))

    return total

# GDocP Framework Analysis Functions
def _analyze_attribution(content: str) -> float:
    """Analisa atribuição."""
    attribution_keywords = ['author', 'written by', 'created by', 'contributor', 'editor', 'reviewer']
    count = sum(1 for keyword in attribution_keywords if keyword.lower() in content.lower())
    return _calculate_score(count, 5)

def _count_attribution_elements(content: str) -> int:
    """Conta elementos de atribuição."""
    attribution_patterns = [
        r'(?:author|written by|created by)',
        r'(?:contributor|editor|reviewer)',
        r'(?:byline|credit|attribution)'
    ]

    total = 0
    for pattern in attribution_patterns:
        total += len(re.findall(pattern, content, re.IGNORECASE))

    return total

def _analyze_readability(content: str) -> float:
    """Analisa legibilidade básica."""
    words = len(content.split())
    sentences = len([s for s in content.split('.') if s.strip()])
    avg_words_per_sentence = words / max(sentences, 1)

    # Score baseado em média de palavras por frase (ideal: 15-20)
    if 15 <= avg_words_per_sentence <= 20:
        return 100
    elif 10 <= avg_words_per_sentence <= 25:
        return 80
    elif 5 <= avg_words_per_sentence <= 30:
        return 60
    else:
        return 40

def _analyze_timeliness(content: str) -> float:
    """Analisa atualidade."""
    time_keywords = ['updated', 'recent', 'latest', 'current', '2025', '2024', 'now', 'today']
    count = sum(1 for keyword in time_keywords if keyword.lower() in content.lower())
    return _calculate_score(count, 6)

def _count_date_references(content: str) -> int:
    """Conta referências de data."""
    date_patterns = [
        r'(?:20\d{2})',
        r'(?:updated|revised|modified)',
        r'(?:January|February|March|April|May|June|July|August|September|October|November|December)'
    ]

    total = 0
    for pattern in date_patterns:
        total += len(re.findall(pattern, content, re.IGNORECASE))

    return total

def _analyze_originality(content: str) -> float:
    """Analisa originalidade."""
    original_keywords = ['original', 'unique', 'novel', 'first', 'new', 'innovative', 'breakthrough']
    count = sum(1 for keyword in original_keywords if keyword.lower() in content.lower())
    return _calculate_score(count, 6)

def _count_original_content(content: str) -> int:
    """Conta conteúdo original."""
    original_patterns = [
        r'(?:original|unique|novel)',
        r'(?:first time|breakthrough|innovative)',
        r'(?:our research|our study|our findings)'
    ]

    total = 0
    for pattern in original_patterns:
        total += len(re.findall(pattern, content, re.IGNORECASE))

    return total

def _analyze_accuracy(content: str) -> float:
    """Analisa precisão."""
    accuracy_keywords = ['accurate', 'precise', 'verified', 'fact', 'evidence', 'proven', 'tested']
    count = sum(1 for keyword in accuracy_keywords if keyword.lower() in content.lower())
    return _calculate_score(count, 6)

def _count_factual_claims(content: str) -> int:
    """Conta afirmações factuais."""
    factual_patterns = [
        r'(?:\d+(?:\.\d+)?%)',
        r'(?:according to|research shows|studies indicate)',
        r'(?:fact|proven|verified|tested)'
    ]

    total = 0
    for pattern in factual_patterns:
        total += len(re.findall(pattern, content, re.IGNORECASE))

    return total

def _analyze_completeness(content: str) -> float:
    """Analisa completude."""
    completeness_keywords = ['comprehensive', 'complete', 'thorough', 'detailed', 'extensive', 'full']
    count = sum(1 for keyword in completeness_keywords if keyword.lower() in content.lower())
    return _calculate_score(count, 6)

def _assess_topic_coverage(content: str) -> List[str]:
    """Avalia cobertura de tópicos."""
    coverage_areas = []

    # Identifica seções/tópicos
    section_patterns = [
        r'(?:## |### |#### )([^\n]+)',
        r'(?:^|\n)(\d+\. [^\n]+)',
        r'(?:\*\*|__)(.*?)(?:\*\*|__)'
    ]

    for pattern in section_patterns:
        matches = re.findall(pattern, content, re.MULTILINE)
        coverage_areas.extend([match.strip() for match in matches[:5]])

    return coverage_areas[:10]

async def _read_resource_async(filename: str) -> dict:
    """Função auxiliar assíncrona para leitura de recursos markdown usando ThreadPoolExecutor."""
    loop = asyncio.get_running_loop()

    # Executa a operação de I/O síncrona em um thread separado
    try:
        result = await loop.run_in_executor(None, _read_resource, filename)
        return result
    except Exception as e:
        logger.error(f"Error in async file reading '{filename}': {str(e)}")
        return {"success": False, "error": f"Async file read error: {str(e)}"}

def _read_resource(filename: str) -> dict:
    """Função auxiliar síncrona para leitura de recursos markdown (fallback)."""
    script_dir = os.path.dirname(os.path.abspath(__file__))

    logger.info(f"Reading resource file (sync): {filename}")

    # Validate filename
    if not filename or not filename.strip():
        logger.warning("Empty filename provided to _read_resource")
        raise FileOperationError("Filename cannot be empty")

    # Prevent path traversal attacks
    if '..' in filename or '/' in filename or '\\' in filename:
        logger.warning(f"Path traversal attempt detected in filename: {filename}")
        raise FileOperationError("Invalid filename - path traversal not allowed")

    try:
        file_path = os.path.join(script_dir, filename)

        # Check if file exists before opening
        if not os.path.exists(file_path):
            logger.error(f"File not found: {filename} at path {file_path}")
            raise FileOperationError(f"Required file '{filename}' not found")

        # Check file size (prevent reading extremely large files)
        file_size = os.path.getsize(file_path)
        max_size = Config.MAX_FILE_SIZE_BYTES
        if file_size > max_size:
            logger.warning(f"Large file detected: {filename} ({file_size} bytes)")
            raise FileOperationError(f"File '{filename}' is too large ({file_size} bytes, max {max_size})")

        logger.debug(f"File validation passed for {filename}: size={file_size} bytes")

        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()

            # Validate content
            if not content.strip():
                logger.warning(f"File '{filename}' is empty")

            logger.info(f"Successfully read {filename}: {len(content)} characters")
            return {"success": True, "data": {"content": content}}

    except FileOperationError:
        raise  # Re-raise our custom exceptions
    except UnicodeDecodeError as e:
        logger.error(f"Encoding error reading {filename}: {str(e)}")
        raise FileOperationError(f"File '{filename}' encoding error: {str(e)}")
    except PermissionError as e:
        logger.error(f"Permission denied reading {filename}: {str(e)}")
        raise FileOperationError(f"Permission denied reading '{filename}': {str(e)}")
    except Exception as e:
        logger.error(f"Unexpected error reading {filename}: {str(e)}")
        raise FileOperationError(f"Unexpected error reading '{filename}': {str(e)}")

async def _get_cached_content_async(filename: str) -> dict:
    """Obtém conteúdo com cache LRU otimizado usando I/O assíncrono."""
    logger.debug(f"Requesting cached content (async): {filename}")

    cached_result = CONTENT_CACHE.get(filename)
    if cached_result is None:
        logger.debug(f"Cache miss for {filename}, reading asynchronously")
        result = await _read_resource_async(filename)
        CONTENT_CACHE.set(filename, result)
        logger.info(f"Cached content for {filename} successfully (async)")
        return result

    logger.debug(f"Cache hit for {filename} (async)")
    return cached_result

def _get_cached_content(filename: str) -> dict:
    """Obtém conteúdo com cache LRU otimizado (fallback síncrono)."""
    logger.debug(f"Requesting cached content (sync): {filename}")

    cached_result = CONTENT_CACHE.get(filename)
    if cached_result is None:
        logger.debug(f"Cache miss for {filename}, reading synchronously")
        result = _read_resource(filename)
        CONTENT_CACHE.set(filename, result)
        logger.info(f"Cached content for {filename} successfully (sync)")
        return result

    logger.debug(f"Cache hit for {filename} (sync)")
    return cached_result

def _create_config_note(config: Dict[str, Any]) -> str:
    """Cria uma nota formatada em markdown a partir de um dicionário de configuração."""
    note_items = [f"- {key.replace('_', ' ').title()}: {value}" for key, value in config.items()]
    return "\n\n---\n**Configuration Applied:**\n" + "\n".join(note_items) + "\n"

@mcp.tool()
@handle_exceptions
async def health_check() -> dict:
    """Comprehensive health check with detailed system metrics and performance data."""
    logger.info("Performing comprehensive health check")

    import time
    import sys
    import os

    start_time = time.time()

    # Test file access to critical resources
    resource_health = {}
    critical_files = [
        "codes-llm.md",
        "frameworks-marketing-2025.md",
        "technical-writing-2025.md",
        "seo-frameworks-2025.md"
    ]

    for filename in critical_files:
        try:
            result = await _get_cached_content_async(filename)
            resource_health[filename] = {
                "status": "healthy" if result["success"] else "error",
                "accessible": result["success"],
                "cached": filename in CONTENT_CACHE.cache
            }
        except Exception as e:
            resource_health[filename] = {
                "status": "error",
                "accessible": False,
                "error": str(e)
            }

    # Cache performance metrics
    cache_stats = CONTENT_CACHE.get_stats()
    cache_health = {
        "status": "healthy" if cache_stats["hit_ratio"] > 50 else "warning",
        "performance": "excellent" if cache_stats["hit_ratio"] > 80 else
                      "good" if cache_stats["hit_ratio"] > 60 else "needs_attention",
        **cache_stats
    }

    # System resource monitoring (basic metrics without external dependencies)
    try:
        import resource
        import gc

        # Get memory usage info
        memory_usage = resource.getrusage(resource.RUSAGE_SELF)

        system_metrics = {
            "python_version": sys.version.split()[0],
            "platform": sys.platform,
            "memory_peak_mb": round(memory_usage.ru_maxrss / 1024 / 1024, 2) if sys.platform != 'darwin' else round(memory_usage.ru_maxrss / 1024 / 1024, 2),
            "garbage_collector_objects": len(gc.get_objects()),
            "process_id": os.getpid(),
            "current_working_directory": os.getcwd(),
            "file_descriptors_count": len(os.listdir('/proc/self/fd')) if os.path.exists('/proc/self/fd') else "N/A"
        }
    except Exception as e:
        logger.warning(f"Could not gather system metrics: {str(e)}")
        system_metrics = {
            "python_version": sys.version.split()[0],
            "platform": sys.platform,
            "error": "Advanced system metrics unavailable",
            "reason": str(e)
        }

    # Performance test - measure response time
    response_time = round((time.time() - start_time) * 1000, 2)  # milliseconds

    # Overall system health assessment
    healthy_resources = sum(1 for r in resource_health.values() if r["status"] == "healthy")
    total_resources = len(resource_health)
    health_ratio = healthy_resources / total_resources if total_resources > 0 else 0

    overall_status = (
        "healthy" if health_ratio >= 1.0 and response_time < 500 else
        "warning" if health_ratio >= 0.8 and response_time < 1000 else
        "critical"
    )

    logger.info(f"Health check completed in {response_time}ms - Status: {overall_status}")

    return {
        "status": overall_status,
        "timestamp": datetime.now().isoformat(),
        "response_time_ms": response_time,
        "version": "0.2.2",
        "phase": "Phase 1 - Quick Wins Implementation",

        # Core system info
        "system": {
            "resources": ["osp://marketing-tools"],
            "methodology_versions": METHODOLOGY_VERSIONS,
            "total_methodologies": len(METHODOLOGY_VERSIONS),
            "configurable_tools": 14,  # Updated count
            "total_parameters": 43,
            "error_handling": "Enhanced with custom exceptions",
            "logging": "Structured logging enabled",
            "async_io": "ThreadPoolExecutor implementation"
        },

        # Resource health details
        "resources": {
            "health_ratio": f"{health_ratio:.1%}",
            "details": resource_health
        },

        # Cache performance
        "cache": cache_health,

        # Framework validation
        "framework_validation": {
            "enabled": True,
            "valid_frameworks": list(VALID_FRAMEWORKS),
            "total_frameworks": len(VALID_FRAMEWORKS)
        },

        # System performance
        "performance": {
            "metrics": system_metrics,
            "assessment": {
                "response_time": "excellent" if response_time < 200 else
                               "good" if response_time < 500 else
                               "slow" if response_time < 1000 else "critical",
                "cache_efficiency": cache_health["performance"],
                "resource_availability": f"{healthy_resources}/{total_resources} resources healthy"
            }
        },

        # Implementation status
        "implementation_status": {
            "phase_1_completed": {
                "error_handling": True,
                "structured_logging": True,
                "async_io": True,
                "cache_optimization": True,
                "health_metrics": True
            },
            "next_phase": "Phase 2 - Quality Foundations (Testing & CI/CD)"
        },

        "last_updated": "2025-08-17"
    }

@mcp.tool()
@handle_exceptions
async def get_editing_codes(
    detail_level: str = "standard",  # basic, standard, comprehensive
    output_format: str = "markdown",  # markdown, json, yaml
    target_persona: str = "general",  # technical, marketing, general
    include_examples: bool = True,
    methodology_version: str = "latest"
) -> dict:
    """Get the Open Strategy Partners (OSP) editing codes documentation and usage protocol for editing texts.

    Args:
        detail_level: Level of detail (basic, standard, comprehensive)
        output_format: Output format (markdown, json, yaml)
        target_persona: Target audience (technical, marketing, general)
        include_examples: Whether to include examples
        methodology_version: Version to use (latest, 1.0.0)
    """
    result = await _get_cached_content_async('codes-llm.md')
    if result["success"]:
        configuration = {
            "detail_level": detail_level,
            "output_format": output_format,
            "target_persona": target_persona,
            "include_examples": include_examples,
            "methodology_version": methodology_version
        }
        result["data"]["methodology"] = "osp_editing_codes"
        result["data"]["version"] = METHODOLOGY_VERSIONS["osp_editing_codes"]
        result["data"]["type"] = "editing_methodology"
        result["data"]["configuration"] = configuration
        # Add configuration note to content
        if result["data"]["content"]:
            result["data"]["content"] += _create_config_note(configuration)
    return result

@mcp.tool()
@handle_exceptions
async def get_writing_guide(
    document_type: str = "general",  # tutorial, reference, api, guide, blog
    audience_level: str = "intermediate",  # beginner, intermediate, advanced, expert
    content_focus: str = "comprehensive",  # structure, style, technical, comprehensive
    include_checklists: bool = True,
    language_style: str = "professional"  # casual, professional, academic, conversational
) -> dict:
    """Get the Open Strategy Partners (OSP) writing guide and usage protocol for editing texts.

    Args:
        document_type: Type of document (tutorial, reference, api, guide, blog)
        audience_level: Target audience level (beginner, intermediate, advanced, expert)
        content_focus: Focus area (structure, style, technical, comprehensive)
        include_checklists: Whether to include quality checklists
        language_style: Writing style (casual, professional, academic, conversational)
    """
    result = _get_cached_content('guide-llm.md')
    if result["success"]:
        configuration = {
            "document_type": document_type,
            "audience_level": audience_level,
            "content_focus": content_focus,
            "include_checklists": include_checklists,
            "language_style": language_style
        }
        result["data"]["methodology"] = "osp_writing_guide"
        result["data"]["version"] = METHODOLOGY_VERSIONS["osp_writing_guide"]
        result["data"]["type"] = "writing_methodology"
        result["data"]["configuration"] = configuration
        # Add configuration note to content
        if result["data"]["content"]:
            result["data"]["content"] += _create_config_note(configuration)
    return result

@mcp.tool()
@handle_exceptions
async def get_meta_guide(
    content_type: str = "article",  # article, landing-page, product, service, blog
    seo_focus: str = "balanced",  # keywords, user-intent, technical, balanced
    target_length: str = "standard",  # short, standard, long
    industry: str = "technology",  # technology, healthcare, finance, ecommerce, general
    optimization_goal: str = "ctr"  # ctr, rankings, conversions, brand
) -> dict:
    """Get the Open Strategy Partners (OSP) Web Content Meta Information Generation System (titles, meta-titles, slugs).

    Args:
        content_type: Type of content (article, landing-page, product, service, blog)
        seo_focus: SEO optimization focus (keywords, user-intent, technical, balanced)
        target_length: Target content length (short, standard, long)
        industry: Target industry (technology, healthcare, finance, ecommerce, general)
        optimization_goal: Primary goal (ctr, rankings, conversions, brand)
    """
    result = _get_cached_content('meta-llm.md')
    if result["success"]:
        configuration = {
            "content_type": content_type,
            "seo_focus": seo_focus,
            "target_length": target_length,
            "industry": industry,
            "optimization_goal": optimization_goal
        }
        result["data"]["methodology"] = "osp_meta_guide"
        result["data"]["version"] = METHODOLOGY_VERSIONS["osp_meta_guide"]
        result["data"]["type"] = "meta_optimization"
        result["data"]["configuration"] = configuration
        # Add configuration note to content
        if result["data"]["content"]:
            result["data"]["content"] += _create_config_note(configuration)
    return result

@mcp.tool()
@handle_exceptions
async def get_value_map_positioning_guide(
    product_type: str = "saas",  # saas, hardware, service, platform, api
    market_stage: str = "growth",  # startup, growth, mature, enterprise
    complexity_level: str = "standard",  # simple, standard, complex, enterprise
    target_market: str = "b2b",  # b2b, b2c, b2b2c, marketplace
    positioning_focus: str = "comprehensive"  # features, benefits, differentiation, comprehensive
) -> dict:
    """Get the Open Strategy Partners (OSP) Product Communications Value Map Generation System for Product Positioning.

    Args:
        product_type: Type of product (saas, hardware, service, platform, api)
        market_stage: Market stage (startup, growth, mature, enterprise)
        complexity_level: Product complexity (simple, standard, complex, enterprise)
        target_market: Target market type (b2b, b2c, b2b2c, marketplace)
        positioning_focus: Focus area (features, benefits, differentiation, comprehensive)
    """
    result = _get_cached_content('product-value-map-llm.md')
    if result["success"]:
        configuration = {
            "product_type": product_type,
            "market_stage": market_stage,
            "complexity_level": complexity_level,
            "target_market": target_market,
            "positioning_focus": positioning_focus
        }
        result["data"]["methodology"] = "osp_value_map_guide"
        result["data"]["version"] = METHODOLOGY_VERSIONS["osp_value_map_guide"]
        result["data"]["type"] = "positioning_methodology"
        result["data"]["configuration"] = configuration
        # Add configuration note to content
        if result["data"]["content"]:
            result["data"]["content"] += _create_config_note(configuration)
    return result

@mcp.tool()
@handle_exceptions
async def get_on_page_seo_guide(
    focus_area: str = "comprehensive",  # on-page, technical, content, local, comprehensive
    industry: str = "technology",  # technology, ecommerce, healthcare, finance, local-business
    difficulty: str = "intermediate",  # beginner, intermediate, advanced, expert
    checklist_format: bool = False,
    include_tools: bool = True,
    seo_framework: str = "traditional"  # traditional, e-eat, entity-based, core-vitals
) -> dict:
    """Get the Open Strategy Partners (OSP) On-Page SEO Optimization Guide.

    Args:
        focus_area: SEO focus area (on-page, technical, content, local, comprehensive)
        industry: Target industry (technology, ecommerce, healthcare, finance, local-business)
        difficulty: Complexity level (beginner, intermediate, advanced, expert)
        checklist_format: Return as actionable checklist format
        include_tools: Include tool recommendations
        seo_framework: SEO framework approach (traditional, e-eat, entity-based, core-vitals)
    """
    result = _get_cached_content('on-page-seo-guide.md')
    if result["success"]:
        configuration = {
            "focus_area": focus_area,
            "industry": industry,
            "difficulty": difficulty,
            "checklist_format": checklist_format,
            "include_tools": include_tools,
            "seo_framework": seo_framework
        }
        result["data"]["methodology"] = "osp_seo_guide"
        result["data"]["version"] = METHODOLOGY_VERSIONS["osp_seo_guide"]
        result["data"]["type"] = "seo_methodology"
        result["data"]["configuration"] = configuration
        # Add configuration note to content
        if result["data"]["content"]:
            result["data"]["content"] += _create_config_note(configuration)
    return result

# ==== NOVAS FERRAMENTAS 2025 ====

@mcp.tool()
@handle_exceptions
async def get_marketing_frameworks_2025(
    framework_focus: str = "all",  # ideal, stepps, race, stp, they-ask-you-answer, all
    content_type: str = "digital",  # digital, traditional, social, email, video, all
    audience_type: str = "b2b",  # b2b, b2c, mixed
    campaign_stage: str = "comprehensive",  # awareness, consideration, conversion, retention, comprehensive
    industry_vertical: str = "technology"  # technology, healthcare, finance, ecommerce, saas, general
) -> dict:
    """Get the 2025 Marketing Content Frameworks including IDEAL, STEPPS, RACE, STP, and They Ask You Answer methodologies.

    Args:
        framework_focus: Specific framework to focus on (ideal, stepps, race, stp, they-ask-you-answer, all)
        content_type: Type of content (digital, traditional, social, email, video, all)
        audience_type: Target audience (b2b, b2c, mixed)
        campaign_stage: Marketing funnel stage (awareness, consideration, conversion, retention, comprehensive)
        industry_vertical: Industry focus (technology, healthcare, finance, ecommerce, saas, general)
    """
    result = _get_cached_content('frameworks-marketing-2025.md')
    if result["success"]:
        configuration = {
            "framework_focus": framework_focus,
            "content_type": content_type,
            "audience_type": audience_type,
            "campaign_stage": campaign_stage,
            "industry_vertical": industry_vertical
        }
        result["data"]["methodology"] = "frameworks_marketing_2025"
        result["data"]["version"] = METHODOLOGY_VERSIONS["frameworks_marketing_2025"]
        result["data"]["type"] = "marketing_frameworks"
        result["data"]["frameworks"] = ["IDEAL", "STEPPS", "RACE", "STP", "They Ask You Answer"]
        result["data"]["year"] = "2025"
        result["data"]["configuration"] = configuration
        # Add configuration note to content
        if result["data"]["content"]:
            result["data"]["content"] += _create_config_note(configuration)
    return result

@mcp.tool()
@handle_exceptions
async def get_technical_writing_2025(
    framework_focus: str = "all",  # gdocp, docs-as-code, interactive, content-design, all
    documentation_type: str = "api",  # api, user-guide, reference, tutorial, troubleshooting
    complexity_level: str = "intermediate",  # basic, intermediate, advanced, expert
    team_size: str = "medium",  # solo, small, medium, large, enterprise
    automation_level: str = "standard"  # minimal, standard, advanced, full
) -> dict:
    """Get the 2025 Technical Writing Methodologies including GDocP (ALCOA-C), Docs-as-Code, and Interactive Documentation frameworks.

    Args:
        framework_focus: Specific framework focus (gdocp, docs-as-code, interactive, content-design, all)
        documentation_type: Type of documentation (api, user-guide, reference, tutorial, troubleshooting)
        complexity_level: Technical complexity (basic, intermediate, advanced, expert)
        team_size: Team size context (solo, small, medium, large, enterprise)
        automation_level: Desired automation level (minimal, standard, advanced, full)
    """
    result = _get_cached_content('technical-writing-2025.md')
    if result["success"]:
        configuration = {
            "framework_focus": framework_focus,
            "documentation_type": documentation_type,
            "complexity_level": complexity_level,
            "team_size": team_size,
            "automation_level": automation_level
        }
        result["data"]["methodology"] = "technical_writing_2025"
        result["data"]["version"] = METHODOLOGY_VERSIONS["technical_writing_2025"]
        result["data"]["type"] = "technical_writing"
        result["data"]["frameworks"] = ["GDocP", "Docs-as-Code", "Interactive Documentation", "Content Design System"]
        result["data"]["year"] = "2025"
        result["data"]["configuration"] = configuration
        # Add configuration note to content
        if result["data"]["content"]:
            result["data"]["content"] += _create_config_note(configuration)
    return result

@mcp.tool()
@handle_exceptions
async def get_seo_frameworks_2025(
    framework_focus: str = "all",  # e-eat, entity-based, snippets, clusters, core-vitals, all
    optimization_goal: str = "rankings",  # rankings, traffic, conversions, authority, user-experience
    content_maturity: str = "existing",  # new, existing, refresh, comprehensive
    technical_level: str = "intermediate",  # basic, intermediate, advanced, expert
    search_intent: str = "mixed"  # informational, navigational, transactional, commercial, mixed
) -> dict:
    """Get the 2025 SEO and Optimization Frameworks including E-E-A-T, Entity-Based SEO, Snippet-Friendly Structure, and Topic Cluster Strategy.

    Args:
        framework_focus: Specific framework focus (e-eat, entity-based, snippets, clusters, core-vitals, all)
        optimization_goal: Primary SEO goal (rankings, traffic, conversions, authority, user-experience)
        content_maturity: Content lifecycle stage (new, existing, refresh, comprehensive)
        technical_level: Technical complexity (basic, intermediate, advanced, expert)
        search_intent: Target search intent (informational, navigational, transactional, commercial, mixed)
    """
    result = _get_cached_content('seo-frameworks-2025.md')
    if result["success"]:
        configuration = {
            "framework_focus": framework_focus,
            "optimization_goal": optimization_goal,
            "content_maturity": content_maturity,
            "technical_level": technical_level,
            "search_intent": search_intent
        }
        result["data"]["methodology"] = "seo_frameworks_2025"
        result["data"]["version"] = METHODOLOGY_VERSIONS["seo_frameworks_2025"]
        result["data"]["type"] = "seo_frameworks"
        result["data"]["frameworks"] = ["E-E-A-T", "Entity-Based SEO", "Snippet-Friendly Structure", "Topic Cluster Strategy", "Core Web Vitals"]
        result["data"]["year"] = "2025"
        result["data"]["configuration"] = configuration
        # Add configuration note to content
        if result["data"]["content"]:
            result["data"]["content"] += _create_config_note(configuration)
    return result

@mcp.tool()
@handle_exceptions
async def get_methodology_versions() -> dict:
    """Get version information for all available methodologies and frameworks."""
    return {
        "success": True,
        "data": {
            "versions": METHODOLOGY_VERSIONS,
            "total_methodologies": len(METHODOLOGY_VERSIONS),
            "last_updated": "2025-08-17",
            "categories": {
                "osp_legacy": ["osp_editing_codes", "osp_writing_guide", "osp_meta_guide", "osp_value_map_guide", "osp_seo_guide"],
                "frameworks_2025": ["frameworks_marketing_2025", "technical_writing_2025", "seo_frameworks_2025"]
            }
        }
    }

@mcp.tool()
@handle_exceptions
async def get_cache_statistics() -> dict:
    """Get detailed cache statistics and performance metrics."""
    cache_stats = CONTENT_CACHE.get_stats()

    # Adicionar análise de performance
    performance_analysis = {
        "efficiency": "excellent" if cache_stats["hit_ratio"] >= 80 else
                     "good" if cache_stats["hit_ratio"] >= 60 else
                     "fair" if cache_stats["hit_ratio"] >= 40 else "poor",
        "memory_usage": "optimal" if cache_stats["utilization"] <= 80 else
                       "high" if cache_stats["utilization"] <= 95 else "critical",
        "recommendations": []
    }

    # Gerar recomendações baseadas nas estatísticas
    if cache_stats["hit_ratio"] < 60:
        performance_analysis["recommendations"].append("Consider increasing cache size for better hit ratio")

    if cache_stats["evictions"] > cache_stats["hits"] * 0.1:
        performance_analysis["recommendations"].append("High eviction rate detected - cache size may be too small")

    if cache_stats["utilization"] > 90:
        performance_analysis["recommendations"].append("Cache utilization is high - monitor for performance impact")

    if not performance_analysis["recommendations"]:
        performance_analysis["recommendations"].append("Cache performance is optimal")

    return {
        "success": True,
        "data": {
            "cache_statistics": cache_stats,
            "performance_analysis": performance_analysis,
            "methodology": "LRU Cache with Statistics",
            "monitoring_timestamp": datetime.now().isoformat()
        }
    }

@mcp.tool()
@handle_exceptions
async def clear_cache_statistics() -> dict:
    """Clear cache statistics (keep cached content but reset metrics)."""
    old_stats = CONTENT_CACHE.get_stats()
    CONTENT_CACHE.clear_stats()
    new_stats = CONTENT_CACHE.get_stats()

    return {
        "success": True,
        "data": {
            "message": "Cache statistics cleared successfully",
            "previous_stats": old_stats,
            "current_stats": new_stats,
            "cleared_timestamp": datetime.now().isoformat()
        }
    }

@mcp.tool()
@handle_exceptions
async def benchmark_file_operations() -> dict:
    """Benchmark sync vs async file operations to demonstrate performance improvements."""
    import time

    test_files = ['codes-llm.md', 'guide-llm.md', 'meta-llm.md']

    # Benchmark synchronous operations
    sync_start = time.time()
    sync_results = []
    for filename in test_files:
        result = _read_resource(filename)
        sync_results.append({"file": filename, "success": result["success"]})
    sync_duration = time.time() - sync_start

    # Benchmark asynchronous operations
    async_start = time.time()
    async_tasks = [_read_resource_async(filename) for filename in test_files]
    async_results_raw = await asyncio.gather(*async_tasks)
    async_results = [{"file": test_files[i], "success": result["success"]}
                    for i, result in enumerate(async_results_raw)]
    async_duration = time.time() - async_start

    # Calculate performance improvement
    improvement = ((sync_duration - async_duration) / sync_duration * 100) if sync_duration > 0 else 0

    return {
        "success": True,
        "data": {
            "benchmark_results": {
                "synchronous": {
                    "duration_seconds": round(sync_duration, 4),
                    "files_processed": len(sync_results),
                    "results": sync_results
                },
                "asynchronous": {
                    "duration_seconds": round(async_duration, 4),
                    "files_processed": len(async_results),
                    "results": async_results
                },
                "performance_improvement": {
                    "time_saved_seconds": round(sync_duration - async_duration, 4),
                    "improvement_percentage": round(improvement, 1),
                    "analysis": "faster" if async_duration < sync_duration else "slower",
                    "note": "Async benefits are more apparent with multiple concurrent requests"
                }
            },
            "methodology": "ThreadPoolExecutor-based async I/O",
            "benchmark_timestamp": datetime.now().isoformat()
        }
    }

@mcp.tool()
@handle_exceptions
async def analyze_content_multi_framework(
    content: str,
    frameworks: List[str] = None
) -> dict:
    """Analyze content using multiple 2025 frameworks with enhanced validation and error handling.

    Args:
        content: The text content to analyze (minimum 10 characters, maximum 1MB)
        frameworks: List of frameworks to use. Available: IDEAL, STEPPS, E-E-A-T, GDocP

    Returns:
        Dict containing analysis results for each framework with scores and recommendations

    Raises:
        ContentValidationError: If content is invalid
        FrameworkValidationError: If frameworks are invalid
    """
    if frameworks is None:
        frameworks = ["IDEAL", "STEPPS", "E-E-A-T"]

    logger.info(f"Starting multi-framework analysis with {len(frameworks)} frameworks")

    # Enhanced content validation with configurable limits
    if not content:
        raise ContentValidationError("Content parameter is required")

    if not content.strip():
        raise ContentValidationError("Content cannot be empty or whitespace only")

    if len(content) < 10:
        raise ContentValidationError("Content is too short for meaningful analysis (minimum 10 characters)")

    if len(content) > Config.MAX_ANALYSIS_CONTENT_LENGTH:
        raise ContentValidationError(f"Content is too large for analysis (maximum {Config.MAX_ANALYSIS_CONTENT_LENGTH} characters)")

    # Framework validation with strict mode support
    if not frameworks:
        raise FrameworkValidationError("At least one framework must be specified")

    if not isinstance(frameworks, list):
        raise FrameworkValidationError("Frameworks must be provided as a list")

    # Validate each framework
    unrecognized_frameworks = []
    processed_frameworks = []

    for framework in frameworks:
        if framework not in VALID_FRAMEWORKS:
            unrecognized_frameworks.append(framework)
            if Config.STRICT_FRAMEWORK_VALIDATION:
                raise FrameworkValidationError(f"Framework '{framework}' is not supported. Available: {list(VALID_FRAMEWORKS)}")
        else:
            processed_frameworks.append(framework)

    # If no valid frameworks and not in strict mode, provide helpful error
    if not processed_frameworks:
        available_frameworks = ", ".join(VALID_FRAMEWORKS)
        raise FrameworkValidationError(f"No valid frameworks found. Available frameworks: {available_frameworks}")

    try:
        # Use the new modular analysis system
        analysis_results = analyze_content_with_frameworks(content, processed_frameworks)

        # Calculate overall scores
        overall_scores = {}
        for framework, results in analysis_results.items():
            if "error" not in results:
                # Extract scores from each framework's analysis
                scores = []
                for section_name, section_data in results.items():
                    if isinstance(section_data, dict) and "score" in section_data:
                        scores.append(section_data["score"])

                if scores:
                    overall_scores[framework] = round(sum(scores) / len(scores), 1)
                else:
                    overall_scores[framework] = 0

        # Calculate average score across all frameworks
        if overall_scores:
            average_score = round(sum(overall_scores.values()) / len(overall_scores), 1)
        else:
            average_score = 0

        # Add warnings for unrecognized frameworks
        warnings = []
        if unrecognized_frameworks:
            warnings.append(f"Unrecognized frameworks ignored: {', '.join(unrecognized_frameworks)}")

        logger.info(f"Analysis completed successfully. Average score: {average_score}")

        return {
            "success": True,
            "data": {
                "content_length": len(content),
                "content_words": len(content.split()),
                "frameworks_analyzed": processed_frameworks,
                "unrecognized_frameworks": unrecognized_frameworks,
                "analysis": {
                    "frameworks": analysis_results,
                    "overall_scores": {
                        **overall_scores,
                        "average_score": average_score
                    }
                },
                "metadata": {
                    "methodology_version": "2025.1",
                    "analysis_timestamp": datetime.now().isoformat(),
                    "configuration": Config.get_env_info(),
                    "validation_info": {
                        "total_frameworks_requested": len(frameworks),
                        "valid_frameworks_processed": len(processed_frameworks),
                        "invalid_frameworks_ignored": len(unrecognized_frameworks),
                        "available_frameworks": list(VALID_FRAMEWORKS),
                        "strict_validation": Config.STRICT_FRAMEWORK_VALIDATION
                    }
                },
                "warnings": warnings if warnings else None
            }
        }

    except Exception as e:
        logger.error(f"Analysis failed: {str(e)}")
        raise ContentValidationError(f"Analysis processing failed: {str(e)}")


def main() -> None:
    """Run the MCP server."""
    try:
        mcp.run()
    except Exception as e:
        print(f"Error starting server: {str(e)}")
        raise

if __name__ == "__main__":
    main()
